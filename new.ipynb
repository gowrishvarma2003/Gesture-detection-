{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image 1/100 to tests/test_00000.png\n",
      "Saved image 2/100 to tests/test_00001.png\n",
      "Saved image 3/100 to tests/test_00002.png\n",
      "Saved image 4/100 to tests/test_00003.png\n",
      "Saved image 5/100 to tests/test_00004.png\n",
      "Saved image 6/100 to tests/test_00005.png\n",
      "Saved image 7/100 to tests/test_00006.png\n",
      "Saved image 8/100 to tests/test_00007.png\n",
      "Saved image 9/100 to tests/test_00008.png\n",
      "Saved image 10/100 to tests/test_00009.png\n",
      "Saved image 11/100 to tests/test_00010.png\n",
      "Saved image 12/100 to tests/test_00011.png\n",
      "Saved image 13/100 to tests/test_00012.png\n",
      "Saved image 14/100 to tests/test_00013.png\n",
      "Saved image 15/100 to tests/test_00014.png\n",
      "Saved image 16/100 to tests/test_00015.png\n",
      "Saved image 17/100 to tests/test_00016.png\n",
      "Saved image 18/100 to tests/test_00017.png\n",
      "Saved image 19/100 to tests/test_00018.png\n",
      "Saved image 20/100 to tests/test_00019.png\n",
      "Saved image 21/100 to tests/test_00020.png\n",
      "Saved image 22/100 to tests/test_00021.png\n",
      "Saved image 23/100 to tests/test_00022.png\n",
      "Saved image 24/100 to tests/test_00023.png\n",
      "Saved image 25/100 to tests/test_00024.png\n",
      "Saved image 26/100 to tests/test_00025.png\n",
      "Saved image 27/100 to tests/test_00026.png\n",
      "Saved image 28/100 to tests/test_00027.png\n",
      "Saved image 29/100 to tests/test_00028.png\n",
      "Saved image 30/100 to tests/test_00029.png\n",
      "Saved image 31/100 to tests/test_00030.png\n",
      "Saved image 32/100 to tests/test_00031.png\n",
      "Saved image 33/100 to tests/test_00032.png\n",
      "Saved image 34/100 to tests/test_00033.png\n",
      "Saved image 35/100 to tests/test_00034.png\n",
      "Saved image 36/100 to tests/test_00035.png\n",
      "Saved image 37/100 to tests/test_00036.png\n",
      "Saved image 38/100 to tests/test_00037.png\n",
      "Saved image 39/100 to tests/test_00038.png\n",
      "Saved image 40/100 to tests/test_00039.png\n",
      "Saved image 41/100 to tests/test_00040.png\n",
      "Saved image 42/100 to tests/test_00041.png\n",
      "Saved image 43/100 to tests/test_00042.png\n",
      "Saved image 44/100 to tests/test_00043.png\n",
      "Saved image 45/100 to tests/test_00044.png\n",
      "Saved image 46/100 to tests/test_00045.png\n",
      "Saved image 47/100 to tests/test_00046.png\n",
      "Saved image 48/100 to tests/test_00047.png\n",
      "Saved image 49/100 to tests/test_00048.png\n",
      "Saved image 50/100 to tests/test_00049.png\n",
      "Saved image 51/100 to tests/test_00050.png\n",
      "Saved image 52/100 to tests/test_00051.png\n",
      "Saved image 53/100 to tests/test_00052.png\n",
      "Saved image 54/100 to tests/test_00053.png\n",
      "Saved image 55/100 to tests/test_00054.png\n",
      "Saved image 56/100 to tests/test_00055.png\n",
      "Saved image 57/100 to tests/test_00056.png\n",
      "Saved image 58/100 to tests/test_00057.png\n",
      "Saved image 59/100 to tests/test_00058.png\n",
      "Saved image 60/100 to tests/test_00059.png\n",
      "Saved image 61/100 to tests/test_00060.png\n",
      "Saved image 62/100 to tests/test_00061.png\n",
      "Saved image 63/100 to tests/test_00062.png\n",
      "Saved image 64/100 to tests/test_00063.png\n",
      "Saved image 65/100 to tests/test_00064.png\n",
      "Saved image 66/100 to tests/test_00065.png\n",
      "Saved image 67/100 to tests/test_00066.png\n",
      "Saved image 68/100 to tests/test_00067.png\n",
      "Saved image 69/100 to tests/test_00068.png\n",
      "Saved image 70/100 to tests/test_00069.png\n",
      "Saved image 71/100 to tests/test_00070.png\n",
      "Saved image 72/100 to tests/test_00071.png\n",
      "Saved image 73/100 to tests/test_00072.png\n",
      "Saved image 74/100 to tests/test_00073.png\n",
      "Saved image 75/100 to tests/test_00074.png\n",
      "Saved image 76/100 to tests/test_00075.png\n",
      "Saved image 77/100 to tests/test_00076.png\n",
      "Saved image 78/100 to tests/test_00077.png\n",
      "Saved image 79/100 to tests/test_00078.png\n",
      "Saved image 80/100 to tests/test_00079.png\n",
      "Saved image 81/100 to tests/test_00080.png\n",
      "Saved image 82/100 to tests/test_00081.png\n",
      "Saved image 83/100 to tests/test_00082.png\n",
      "Saved image 84/100 to tests/test_00083.png\n",
      "Saved image 85/100 to tests/test_00084.png\n",
      "Saved image 86/100 to tests/test_00085.png\n",
      "Saved image 87/100 to tests/test_00086.png\n",
      "Saved image 88/100 to tests/test_00087.png\n",
      "Saved image 89/100 to tests/test_00088.png\n",
      "Saved image 90/100 to tests/test_00089.png\n",
      "Saved image 91/100 to tests/test_00090.png\n",
      "Saved image 92/100 to tests/test_00091.png\n",
      "Saved image 93/100 to tests/test_00092.png\n",
      "Saved image 94/100 to tests/test_00093.png\n",
      "Saved image 95/100 to tests/test_00094.png\n",
      "Saved image 96/100 to tests/test_00095.png\n",
      "Saved image 97/100 to tests/test_00096.png\n",
      "Saved image 98/100 to tests/test_00097.png\n",
      "Saved image 99/100 to tests/test_00098.png\n",
      "Saved image 100/100 to tests/test_00099.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands model\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# Initialize camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1000)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1000)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "image_label = 3\n",
    "hand_type = 'r'\n",
    "\n",
    "# Wait for the camera to warm up\n",
    "for i in range(100):\n",
    "    cap.read()\n",
    "\n",
    "image_count = 0  # Counter for saved images\n",
    "target_count = 100 # Target number of images\n",
    "\n",
    "while cap.isOpened() and image_count < target_count:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # Initialize black_frame in case no hand is detected\n",
    "    h, w, _ = frame.shape\n",
    "    black_frame = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    # If hands are detected, create an image with only the hand landmarks\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw landmarks as white points on the black image\n",
    "            landmark_points = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                landmark_points.append((x, y))\n",
    "                cv2.circle(black_frame, (x, y), 5, (255), -1)  # Draw a filled white circle for each point\n",
    "\n",
    "            # Draw lines connecting the landmarks\n",
    "            for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                start_idx = connection[0]\n",
    "                end_idx = connection[1]\n",
    "                start_point = landmark_points[start_idx]\n",
    "                end_point = landmark_points[end_idx]\n",
    "                cv2.line(black_frame, start_point, end_point, (255), 2)  # Draw white line between points\n",
    "\n",
    "            # Resize to a specified resolution (e.g., 200x200 pixels)\n",
    "            resized_landmarks = cv2.resize(black_frame, (200, 200))\n",
    "\n",
    "            # Save the processed image with a unique filename\n",
    "            # output_path = f\"images/{image_label}/pp_{hand_type}hand_landmarks{image_count:05d}.png\"\n",
    "            output_path = f\"tests/test_{image_count:05d}.png\"\n",
    "\n",
    "            cv2.imwrite(output_path, resized_landmarks)\n",
    "            print(f\"Saved image {image_count + 1}/{target_count} to {output_path}\")\n",
    "            image_count += 1\n",
    "\n",
    "            # Exit loop if the target number of images is reached\n",
    "            if image_count >= target_count:\n",
    "                break\n",
    "\n",
    "    # Show the camera output with only the landmarks and connections\n",
    "    cv2.imshow(\"Landmarks Output - Adjust Your Hand Position\", black_frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close any OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Clean up MediaPipe resources\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predictions for test images:\n",
      "Test image 1: 2\n",
      "Test image 2: 2\n",
      "Test image 3: 5\n",
      "Test image 4: 2\n",
      "Test image 5: 2\n",
      "Test image 6: 2\n",
      "Test image 7: 2\n",
      "Test image 8: 5\n",
      "Test image 9: 5\n",
      "Test image 10: 5\n",
      "Test image 11: 5\n",
      "Test image 12: 5\n",
      "Test image 13: 2\n",
      "Test image 14: 5\n",
      "Test image 15: 5\n",
      "Test image 16: 5\n",
      "Test image 17: 4\n",
      "Test image 18: 2\n",
      "Test image 19: 2\n",
      "Test image 20: 5\n",
      "Test image 21: 5\n",
      "Test image 22: 5\n",
      "Test image 23: 5\n",
      "Test image 24: 5\n",
      "Test image 25: 5\n",
      "Test image 26: 5\n",
      "Test image 27: 5\n",
      "Test image 28: 5\n",
      "Test image 29: 5\n",
      "Test image 30: 5\n",
      "Test image 31: 5\n",
      "Test image 32: 5\n",
      "Test image 33: 5\n",
      "Test image 34: 5\n",
      "Test image 35: 5\n",
      "Test image 36: 5\n",
      "Test image 37: 5\n",
      "Test image 38: 5\n",
      "Test image 39: 5\n",
      "Test image 40: 2\n",
      "Test image 41: 2\n",
      "Test image 42: 2\n",
      "Test image 43: 2\n",
      "Test image 44: 2\n",
      "Test image 45: 2\n",
      "Test image 46: 2\n",
      "Test image 47: 2\n",
      "Test image 48: 2\n",
      "Test image 49: 2\n",
      "Test image 50: 2\n",
      "Test image 51: 2\n",
      "Test image 52: 2\n",
      "Test image 53: 4\n",
      "Test image 54: 4\n",
      "Test image 55: 2\n",
      "Test image 56: 2\n",
      "Test image 57: 2\n",
      "Test image 58: 2\n",
      "Test image 59: 2\n",
      "Test image 60: 2\n",
      "Test image 61: 2\n",
      "Test image 62: 2\n",
      "Test image 63: 2\n",
      "Test image 64: 2\n",
      "Test image 65: 2\n",
      "Test image 66: 2\n",
      "Test image 67: 2\n",
      "Test image 68: 2\n",
      "Test image 69: 2\n",
      "Test image 70: 2\n",
      "Test image 71: 2\n",
      "Test image 72: 2\n",
      "Test image 73: 2\n",
      "Test image 74: 4\n",
      "Test image 75: 4\n",
      "Test image 76: 4\n",
      "Test image 77: 4\n",
      "Test image 78: 5\n",
      "Test image 79: 5\n",
      "Test image 80: 5\n",
      "Test image 81: 2\n",
      "Test image 82: 5\n",
      "Test image 83: 5\n",
      "Test image 84: 2\n",
      "Test image 85: 2\n",
      "Test image 86: 4\n",
      "Test image 87: 2\n",
      "Test image 88: 2\n",
      "Test image 89: 2\n",
      "Test image 90: 2\n",
      "Test image 91: 2\n",
      "Test image 92: 2\n",
      "Test image 93: 2\n",
      "Test image 94: 2\n",
      "Test image 95: 2\n",
      "Test image 96: 2\n",
      "Test image 97: 2\n",
      "Test image 98: 2\n",
      "Test image 99: 2\n",
      "Test image 100: 2\n",
      "Test image 101: 5\n",
      "Test image 102: 5\n",
      "Test image 103: 5\n",
      "Test image 104: 5\n",
      "Test image 105: 5\n",
      "Test image 106: 5\n",
      "Test image 107: 5\n",
      "Test image 108: 5\n",
      "Test image 109: 5\n",
      "Test image 110: 5\n",
      "Test image 111: 5\n",
      "Test image 112: 5\n",
      "Test image 113: 5\n",
      "Test image 114: 5\n",
      "Test image 115: 5\n",
      "Test image 116: 5\n",
      "Test image 117: 5\n",
      "Test image 118: 5\n",
      "Test image 119: 5\n",
      "Test image 120: 5\n",
      "Test image 121: 5\n",
      "Test image 122: 5\n",
      "Test image 123: 5\n",
      "Test image 124: 5\n",
      "Test image 125: 5\n",
      "Test image 126: 5\n",
      "Test image 127: 5\n",
      "Test image 128: 5\n",
      "Test image 129: 5\n",
      "Test image 130: 5\n",
      "Test image 131: 5\n",
      "Test image 132: 5\n",
      "Test image 133: 5\n",
      "Test image 134: 5\n",
      "Test image 135: 5\n",
      "Test image 136: 5\n",
      "Test image 137: 5\n",
      "Test image 138: 5\n",
      "Test image 139: 5\n",
      "Test image 140: 5\n",
      "Test image 141: 5\n",
      "Test image 142: 5\n",
      "Test image 143: 5\n",
      "Test image 144: 5\n",
      "Test image 145: 5\n",
      "Test image 146: 5\n",
      "Test image 147: 5\n",
      "Test image 148: 5\n",
      "Test image 149: 5\n",
      "Test image 150: 5\n",
      "Test image 151: 5\n",
      "Test image 152: 5\n",
      "Test image 153: 5\n",
      "Test image 154: 5\n",
      "Test image 155: 5\n",
      "Test image 156: 5\n",
      "Test image 157: 5\n",
      "Test image 158: 5\n",
      "Test image 159: 5\n",
      "Test image 160: 5\n",
      "Test image 161: 5\n",
      "Test image 162: 5\n",
      "Test image 163: 5\n",
      "Test image 164: 5\n",
      "Test image 165: 5\n",
      "Test image 166: 5\n",
      "Test image 167: 2\n",
      "Test image 168: 5\n",
      "Test image 169: 5\n",
      "Test image 170: 5\n",
      "Test image 171: 2\n",
      "Test image 172: 5\n",
      "Test image 173: 5\n",
      "Test image 174: 5\n",
      "Test image 175: 2\n",
      "Test image 176: 2\n",
      "Test image 177: 2\n",
      "Test image 178: 2\n",
      "Test image 179: 2\n",
      "Test image 180: 5\n",
      "Test image 181: 5\n",
      "Test image 182: 2\n",
      "Test image 183: 2\n",
      "Test image 184: 5\n",
      "Test image 185: 2\n",
      "Test image 186: 2\n",
      "Test image 187: 5\n",
      "Test image 188: 5\n",
      "Test image 189: 2\n",
      "Test image 190: 5\n",
      "Test image 191: 5\n",
      "Test image 192: 2\n",
      "Test image 193: 5\n",
      "Test image 194: 5\n",
      "Test image 195: 5\n",
      "Test image 196: 2\n",
      "Test image 197: 2\n",
      "Test image 198: 2\n",
      "Test image 199: 2\n"
     ]
    }
   ],
   "source": [
    "# test images in tests folder (1000 x 1000 sized)\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "best_model = load_model(r'models/m1.keras')\n",
    "test_images_path = 'tests/'\n",
    "x_test = []\n",
    "for test_single in os.listdir(test_images_path):\n",
    "    img_path = test_images_path + test_single\n",
    "    img = cv2.imread(img_path, 0)\n",
    "    img = cv2.resize(img, (50, 50))\n",
    "    img = img.reshape((50,50,1))\n",
    "    img = img/255.0\n",
    "    x_test.append(img)\n",
    "\n",
    "X_test = np.array(x_test)\n",
    "predictions = best_model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(\"Predictions for test images:\")\n",
    "for i, pred in enumerate(predicted_classes):\n",
    "    print(f\"Test image {i + 1}: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=models/m1.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/m1.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m mp_hands \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n\u001b[0;32m     12\u001b[0m hands \u001b[38;5;241m=\u001b[39m mp_hands\u001b[38;5;241m.\u001b[39mHands(static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, max_num_hands\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gowrish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:193\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    190\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    211\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=models/m1.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import Counter\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('models/m1.keras')\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1000)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1000)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "for i in range(100):\n",
    "    cap.read()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "prediction_text = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(rgb_frame)\n",
    "    h, w, _ = frame.shape\n",
    "    black_frame = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            landmark_points = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                landmark_points.append((x, y))\n",
    "                cv2.circle(black_frame, (x, y), 5, (255), -1)\n",
    "\n",
    "            for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                start_idx = connection[0]\n",
    "                end_idx = connection[1]\n",
    "                start_point = landmark_points[start_idx]\n",
    "                end_point = landmark_points[end_idx]\n",
    "                cv2.line(black_frame, start_point, end_point, (255), 2)\n",
    "\n",
    "            resized_landmarks = cv2.resize(black_frame, (200, 200))\n",
    "            cv2.imwrite(\"image.png\", resized_landmarks)\n",
    "            input_image = cv2.resize(resized_landmarks, (50, 50))\n",
    "            input_image = input_image.reshape((50,50,1))\n",
    "            input_image = input_image / 255.0\n",
    "            input_images = np.array([input_image])\n",
    "\n",
    "            mpredictions = model.predict(input_images)\n",
    "            mpredicted_classes = np.argmax(mpredictions, axis=1)\n",
    "            predictions.append(mpredicted_classes[0])\n",
    "\n",
    "            if len(predictions) == 30:\n",
    "                most_common_prediction = Counter(predictions).most_common(1)[0][0]\n",
    "                predictions = []\n",
    "                prediction_text = f\"Prediction: {most_common_prediction}\"\n",
    "\n",
    "    if prediction_text:\n",
    "        cv2.putText(frame, prediction_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"Camera Output - Adjust Your Hand Position\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
